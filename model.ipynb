{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7cbca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dadcd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LagTransformer(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model, n_heads, dropout):\n",
    "        self.d_model, self.n_heads, self.dropout = d_model, n_heads, nn.Dropout(dropout)\n",
    "        self.query, self.key, self.values = nn.Linear(d_model, d_model, bias = False), nn.Linear(d_model, d_model, bias = False),nn.Linear(d_model, d_model, bias = False)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "    def forward(self, q,k,v, mask):\n",
    "        h = self.d_model//self.n_heads\n",
    "        B = src.shape[0]\n",
    "        Q = self.query(q).view(B,-1,self.n_heads,h).permute(0,2,1,3)\n",
    "        K = self.key(k).view(B,-1,self.n_heads,h).permute(0,2,1,3)\n",
    "        V = self.values(v).view(B,-1,self.n_heads,h).permute(0,2,1,3)\n",
    "        energy = Q @ K.permute(0,1,3,2)\n",
    "        if mask is not None :\n",
    "            energy =  energy.masked_fill(mask==0, -1e20)\n",
    "        A = self.softmax(energy)\n",
    "        C = (A@V).permute(0,2,1,3).reshape(B,-1,self.d_model)\n",
    "        return C\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model, n_heads, dropout, hidden_dim):\n",
    "        self.d_model, self.n_heads, self.dropout = d_model, n_heads, nn.Dropout(dropout)\n",
    "        self.mha = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "        )\n",
    "        self.ln1, self.ln2= nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
    "    def forward(self, src, src_mask):\n",
    "        src_mha = self.mha(src,src,src,mask)\n",
    "        src = self.ln1(self.dropout(src)+ src_mha)\n",
    "        src_ffn = self.ffn(src)\n",
    "        out = self.ln2(src_ffn + self.dropout(src))\n",
    "        return out \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,d_model, n_heads, dropout, hidden_dim, max_length ,n_blocks, device):\n",
    "        self.d_model, self.n_heads, self.dropout = d_model, n_heads, nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList(\n",
    "            DecoderLayer(d_model, n_heads, dropout, hidden_dim) for _ in range(n_blocks)\n",
    "        )\n",
    "        self.pos_emb = nn.Embedding(max_length, d_model)\n",
    "    def forward(self, src, src_mask): ## src is already in the form (B,N,d_model), the embeddings are the lag values \n",
    "        B,N = src.shape[0], src.shape[1]\n",
    "        pos_emb = torch.arange(0,N).expand(B,N)\n",
    "        src = pos_emb + src\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src \n",
    "\n",
    "\n",
    "def lag_emb(src, lagged_list):\n",
    "    \"\"\"\n",
    "    src : N --> out : (N, d_model) with out[i,:]= [src[i-lagged_list[0]], ..., src[i-lagged_list[-1]]]\n",
    "    \"\"\"\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de217d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (informer_env)",
   "language": "python",
   "name": "informer_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
